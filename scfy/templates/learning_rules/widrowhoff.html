<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Widrow-Hoff Learning Rule</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            margin: 0;
            padding: 0;
        }

        header {
            background-color: black;
            color: #f09819;
            text-align: center;
            padding: 10px 0;
        }

        h1 {
            font-size: 24px;
        }

        p {
            font-size: 18px;
            margin: 20px;
        }

        code {
        font-family: Consolas, monospace;
        background-color: #f4f4f4;
        padding: 15px;
        border: 1px solid #ccc;
        display: block;
        margin: 10px 50px;
        white-space: pre-wrap; /* Wrap long lines */
        overflow-x: auto; /* Add horizontal scroll if needed */
        color: #333; /* Text color for the code */
    }
        /* Style for the logout button */
    .logout-button {
        background-color: #f44336; /* Red color */
        color: #fff;
        border-radius: 5px;
        margin: 10px 50px;
        padding: 10px;
        text-decoration: none;
    }

    /* Style for the logout button on hover */
    .logout-button:hover {
        background-color: #d32f2f; /* Darker red color on hover */
    }
    </style>
</head>
<body>
    <header>
        <h1>Widrow-Hoff Learning Rule</h1>
    </header>

    <main>
        <p>
            The Widrow-Hoff Learning Rule, also known as the Least Mean Squares (LMS) algorithm, is a fundamental concept in the field of machine learning and neural networks. It was developed by Bernard Widrow and his graduate student Marcian "Ted" Hoff at Stanford University in the early 1960s. This learning rule is particularly relevant to adaptive filtering and supervised learning tasks, where it plays a crucial role in adjusting the weights of a linear neuron, such as a perceptron or an artificial neuron in a neural network.
            <br>
            <br>
            At its core, the Widrow-Hoff Learning Rule is designed to minimize the error between the predicted output of a model and the actual target value by iteratively adjusting the model's weights. The rule leverages gradient descent to update the weights in the direction that reduces the mean squared error between the predicted and target values. By calculating the derivative of the mean squared error with respect to the model's weights, the algorithm adjusts the weights in a way that brings the model's predictions closer to the desired output.
            <br>
            <br>     
            One of the key advantages of the Widrow-Hoff Learning Rule is its simplicity and effectiveness for linear models. It has found applications in various fields, including signal processing, adaptive filtering, and linear regression. Furthermore, its basic principles are foundational to the training of more complex neural networks, such as multi-layer perceptrons and deep learning models. While it may not be the optimal choice for all machine learning tasks, the Widrow-Hoff Learning Rule provides valuable insights into how models can be trained to adapt to their input data, making it an important building block in the world of artificial intelligence and machine learning.<br>
<br>
<br>
Example Code:        
</p>
        <code>
from sklearn.datasets import make_blobs
from sklearn.linear_model import WidrowHoff
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=0)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train a WidrowHoff classifier
clf = WidrowHoff(max_iter=100, random_state=0)
clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy * 100:.2f}%')

# Plot the data points and decision boundary
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()
xx, yy = plt.meshgrid(
    np.linspace(xlim[0], xlim[1], 50),
    np.linspace(ylim[0], ylim[1], 50)
)
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contour(xx, yy, Z, colors='k', levels=[0], linestyles=['-'])
plt.title('WidrowHoff Decision Boundary')
plt.show()

        </code>
        <br>
        <br>
        <br>
        <center><a href="{% url 'contribute' rule_name=rule %}" class="logout-button">Contribute to the code</a></center>
        <br>
        <br>
    </main>
</body>
</html>
