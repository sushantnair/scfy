<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Hebbian Learning Rule</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            margin: 0;
            padding: 0;
        }

        header {
            background-color: black;
            color: #f09819;
            text-align: center;
            padding: 10px 0;
        }

        h1 {
            font-size: 24px;
        }

        p {
            font-size: 18px;
            margin: 20px;
        }
        pre {
            white-space: pre-wrap; /* This CSS property preserves formatting and line breaks */
        }

        code {
        font-family: Consolas, monospace;
        background-color: #f4f4f4;
        padding: 15px;
        border: 1px solid #ccc;
        display: block;
        margin: 10px 50px;
        white-space: pre-wrap; /* Wrap long lines */
        overflow-x: auto; /* Add horizontal scroll if needed */
        color: #333; /* Text color for the code */
    }
        /* Style for the logout button */
    .logout-button {
        background-color: #f44336; /* Red color */
        color: #fff;
        border-radius: 5px;
        margin: 10px 50px;
        padding: 10px;
        text-decoration: none;
    }

    /* Style for the logout button on hover */
    .logout-button:hover {
        background-color: #d32f2f; /* Darker red color on hover */
    }
    </style>
</head>
<body>
    <header>
        <h1>Hebbian Learning Rule</h1>
    </header>

    <main>
        <p>
            Hebbian learning, named after the Canadian psychologist Donald Hebb, is a fundamental concept in the field of neuroscience and artificial neural networks. This learning rule, often summarized by the phrase "cells that fire together wire together," is a foundational principle for understanding how the connections between neurons in the brain are strengthened through experience. It represents a crucial mechanism behind synaptic plasticity, which is the brain's ability to rewire itself based on learning and experience.
            <br>
            <br>
            In Hebbian learning, the fundamental idea is that when two neurons are active at the same time and are involved in the same task or experience, the synaptic connection between them is strengthened. This reinforcement of synaptic connections is thought to underlie the formation of memory and the adaptation of the neural network to environmental stimuli. Hebbian learning is a mechanism that allows the brain to enhance connections that are repeatedly active, promoting the storage and retrieval of information.
            <br>
            <br>
            While Hebbian learning is a foundational concept in neuroscience, it has also influenced the development of artificial neural networks. In the context of artificial intelligence and machine learning, variants of Hebbian learning have been used to train models to recognize patterns and learn from data. These algorithms often adapt and strengthen the connections or weights between artificial neurons based on the co-occurrence of input patterns. While Hebbian learning has its limitations, especially in complex learning tasks, it provides a valuable starting point for understanding how networks can self-organize and adapt to their input, both in biological and artificial systems.<br>
<br>
Example Code:        
</p>
<code><pre> {{ code|safe}} </pre></code>
        <br>
        <br>
        <br>
        <center><a href="{% url 'contribute' rule_name=rule %}" class="logout-button">Contribute to the code</a></center>
        <br>
        <br>
    </main>
</body>
</html>
