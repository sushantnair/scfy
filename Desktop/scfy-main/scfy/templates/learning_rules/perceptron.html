<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Perceptron Learning Rule</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            margin: 0;
            padding: 0;
        }

        header {
            background-color: black;
            color: #f09819;
            text-align: center;
            padding: 10px 0;
        }

        h1 {
            font-size: 24px;
        }

        p {
            font-size: 18px;
            margin: 20px;
        }

        code {
        font-family: Consolas, monospace;
        background-color: #f4f4f4;
        padding: 15px;
        border: 1px solid #ccc;
        display: block;
        margin: 10px 50px;
        white-space: pre-wrap; /* Wrap long lines */
        overflow-x: auto; /* Add horizontal scroll if needed */
        color: #333; /* Text color for the code */
    }
        /* Style for the logout button */
    .logout-button {
        background-color: #f44336; /* Red color */
        color: #fff;
        border-radius: 5px;
        margin: 10px 50px;
        padding: 10px;
        text-decoration: none;
    }

    /* Style for the logout button on hover */
    .logout-button:hover {
        background-color: #d32f2f; /* Darker red color on hover */
    }
    </style>
</head>
<body>
    <header>
        <h1>Perceptron Learning Rule</h1>
    </header>

    <main>
        <p>
            The Perceptron Learning Rule, often considered the cornerstone of neural network models and an essential concept in the field of machine learning, represents a fundamental algorithm for binary classification tasks. Conceived by Frank Rosenblatt in the late 1950s, the Perceptron is a simple yet powerful model inspired by the way biological neurons function in the human brain. It serves as a building block for more complex artificial neural networks.
            <br>
            <br>
At its core, the Perceptron is a binary linear classifier. It processes input features and applies a set of weights to these features to make a decision about which of two classes an input belongs to. The key idea behind the Perceptron is to find a linear decision boundary that separates data points of one class from those of another. When an input vector is passed through the Perceptron, it calculates the weighted sum of the inputs and applies an activation function (usually a step function). If the result is above a certain threshold, it classifies the input as one class; otherwise, it classifies it as the other class.
            <br>
            <br>
The real power of the Perceptron Learning Rule lies in its ability to learn from data. During training, the Perceptron adjusts its weights based on the observed classification errors. It iteratively fine-tunes these weights to minimize classification errors until convergence. This process of weight adjustment allows the Perceptron to adapt to the underlying patterns in the data and learn the optimal decision boundary that separates the two classes.
<br>
<br>
Example Code:        
</p>
        <code>
from sklearn.datasets import make_blobs
from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Generate synthetic data
X, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=0)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train a Perceptron classifier
clf = Perceptron(max_iter=100, random_state=0)
clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = (y_pred == y_test).mean()
print(f'Accuracy: {accuracy * 100:.2f}%')

# Plot the data points and decision boundary
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()
xx, yy = plt.meshgrid(
    np.linspace(xlim[0], xlim[1], 50),
    np.linspace(ylim[0], ylim[1], 50)
)
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contour(xx, yy, Z, colors='k', levels=[0], linestyles=['-'])
plt.title('Perceptron Decision Boundary')
plt.show()

        </code>
        <br>
        <br>
        <br>
        <center><a href="{% url 'contribute' rule_name=rule %}" class="logout-button">Contribute to the code</a></center>
        <br>
        <br>
    </main>
</body>
</html>
